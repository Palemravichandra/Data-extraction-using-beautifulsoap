{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palemravichandra/Data-extraction-using-beautifulsoap/blob/main/Assaignement_flair_insights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "rMaPj1WNszxs",
        "outputId": "8a02ea2b-831c-49cd-d638-f5abac7bdce5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.19.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.11.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile Ravi.py\n",
        "import csv\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Function to scrape data from a webpage\n",
        "def scrape_webpage(url):\n",
        "    # Start Selenium WebDriver\n",
        "    service = Service('path/to/chromedriver')  # Provide path to your chromedriver\n",
        "    driver = webdriver.Chrome(service=service)\n",
        "\n",
        "    # Open the URL\n",
        "    driver.get(url)\n",
        "\n",
        "    # Get the page source\n",
        "    page_source = driver.page_source\n",
        "\n",
        "    # Close the WebDriver\n",
        "    driver.quit()\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "    # Find relevant information (example: text, images, links)\n",
        "    # Here, we are just finding all the text within <p> tags as an example\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Return the extracted data\n",
        "    return [p.get_text() for p in paragraphs]\n",
        "\n",
        "# Function to read URLs from an Excel file\n",
        "# Function to read URLs from an Excel file without column names and convert to a list\n",
        "def read_urls_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path, header=None)  # Read without column names\n",
        "    urls_list = df[0].tolist()  # Assuming the URLs are in the first column\n",
        "    return urls_list\n",
        "\n",
        "# Function to store scraped data in a CSV file\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Website', 'Text'])  # Write header row\n",
        "        for website, text in data.items():\n",
        "            writer.writerow([website, text])\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Path to the Excel file containing URLs\n",
        "    excel_file_path = '/content/Scrapping Python Assigment- Flair Insights.xlsx'\n",
        "\n",
        "    # Read URLs from the Excel file\n",
        "    urls = read_urls_from_excel(excel_file_path)\n",
        "\n",
        "    # Dictionary to store scraped data\n",
        "    scraped_data = {}\n",
        "\n",
        "    # Scrape data from each URL\n",
        "    for url in urls:\n",
        "        # Extract domain name from URL\n",
        "        domain = urlparse(url).netloc\n",
        "        print(\"Scraping\", domain)\n",
        "\n",
        "        # Scrape data from the webpage\n",
        "        try:\n",
        "            data = scrape_webpage(url)\n",
        "            scraped_data[domain] = ' '.join(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {domain}: {e}\")\n",
        "\n",
        "    # Save scraped data to a CSV file\n",
        "    save_to_csv(scraped_data, 'scraped_data.csv')\n",
        "    print(\"Data scraped and saved to 'scraped_data.csv' successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "sAFoskj4sodR",
        "outputId": "d3ef5c75-df50-4f3b-e9f3-62c754680d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Ravi.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}